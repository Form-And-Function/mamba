{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E = embed_dim\n",
    "# N = latent_dim\n",
    "\n",
    "class S6Block(nn.Module):\n",
    "    def __init__(self, embed_dim, latent_dim, d, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.A_log = nn.Parameter(torch.log(torch.randn(embed_dim, latent_dim)))\n",
    "        self.BW = nn.Parameter(torch.randn(embed_dim, latent_dim))\n",
    "        self.CW = nn.Parameter(torch.randn(embed_dim, latent_dim))\n",
    "\n",
    "        self.DeltaW1 = nn.Parameter(torch.randn(embed_dim, d))\n",
    "        self.DeltaW2 = nn.Parameter(torch.randn(embed_dim, d))\n",
    "\n",
    "        self.DeltaBias = nn.Parameter(torch.randn(embed_dim))\n",
    "\n",
    "        self.WD = nn.Parameter(torch.ones(embed_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, embed_dim)\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        DeltaW = einops.einsum(self.DeltaW1, self.DeltaW2, 'embed_dim1 d, embed_dim2 d -> embed_dim1 embed_dim2')\n",
    "        A = -torch.exp(self.A_log) # (embed_dim, latent_dim)\n",
    "\n",
    "        B = einops.einsum(x, self.BW, 'batch seq embed_dim, embed_dim latent_dim -> batch seq latent_dim')\n",
    "        C = einops.einsum(x, self.CW, 'batch seq embed_dim, embed_dim latent_dim -> batch seq latent_dim')\n",
    "\n",
    "        xDeltaW = einops.einsum(x, DeltaW, 'batch seq embed_dim1, embed_dim2 embed_dim1 -> batch seq embed_dim2')\n",
    "        Delta = F.softplus(xDeltaW + self.DeltaBias) # (batch, seq, embed_dim)\n",
    "\n",
    "        Delta_repeated = einops.repeat(Delta, 'batch seq embed_dim -> batch seq embed_dim latent_dim', latent_dim = self.latent_dim)\n",
    "\n",
    "        A_repeated = einops.repeat(A, 'embed_dim latent_dim -> batch seq embed_dim latent_dim', batch = x.size(0), seq = seq_len) # (batch, seq, embed_dim, latent_dim)\n",
    "        B_repeated = einops.repeat(B, 'batch seq latent_dim -> batch seq embed_dim latent_dim', embed_dim = self.embed_dim) # (batch, seq, embed_dim, latent_dim)\n",
    "\n",
    "        A_bar = A_repeated * Delta_repeated # (batch, seq, embed_dim, latent_dim)\n",
    "        B_bar = B_repeated * Delta_repeated # (batch, seq, embed_dim, latent_dim)\n",
    "\n",
    "        x_by_token = einops.rearrange(x, 'batch seq embed_dim -> seq batch embed_dim')\n",
    "        A_bar_by_token = einops.rearrange(A_bar, 'batch seq embed_dim latent_dim -> seq batch embed_dim latent_dim')\n",
    "        B_bar_by_token = einops.rearrange(B_bar, 'batch seq embed_dim latent_dim -> seq batch embed_dim latent_dim')\n",
    "        C_by_token = einops.rearrange(C, 'batch seq latent_dim -> seq batch latent_dim')\n",
    "\n",
    "        hs = [torch.zeros(batch_size, self.embed_dim, self.latent_dim, device = x.device)]\n",
    "        ys = []\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            token = x_by_token[i]\n",
    "            A_bar_t = A_bar_by_token[i]\n",
    "            B_bar_t = B_bar_by_token[i]\n",
    "            C_t = C_by_token[i]\n",
    "\n",
    "            reshaped_token = einops.repeat(token, 'batch embed_dim -> batch embed_dim latent_dim', latent_dim = 1)\n",
    "            \n",
    "            # print(\"Sizes: \", B_bar_t.size(), token.size())\n",
    "            # print((B_bar_t * reshaped_token).shape)\n",
    "            h = A_bar_t * hs[-1] + B_bar_t * reshaped_token # (batch, embed_dim, latent_dim)\n",
    "            y = einops.einsum(C_t, h, 'batch latent_dim, batch embed_dim latent_dim -> batch embed_dim')\n",
    "\n",
    "            hs.append(h)\n",
    "            ys.append(y)\n",
    "\n",
    "        ys = torch.stack(ys, dim = 0) # (seq, batch, embed_dim)\n",
    "        y = einops.rearrange(ys, 'seq batch embed_dim -> batch seq embed_dim')\n",
    "        y += x * self.WD # skip connection\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# test the S6Block\n",
    "\n",
    "testX = torch.randn(2, 3, 4)\n",
    "print(testX.shape)\n",
    "\n",
    "block = S6Block(4, 5, 2)\n",
    "print(block(testX).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, outer_dim, embed_dim, latent_dim, d, kernel_size, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.project_1 = nn.Linear(outer_dim, embed_dim, bias=False)\n",
    "        self.project_2 = nn.Linear(outer_dim, embed_dim, bias=False)\n",
    "        self.project_3 = nn.Linear(embed_dim, outer_dim, bias=False)\n",
    "\n",
    "        self.s6_block = S6Block(embed_dim, latent_dim, d, dropout = dropout)\n",
    "        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size = kernel_size, groups=embed_dim, padding = kernel_size - 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # print(\"Input: \", x.shape)\n",
    "        # print(\"Project 1: \", self.project_1(x).shape)\n",
    "        projx1 = self.project_1(x)\n",
    "        projx1 = self.dropout(projx1)\n",
    "        projx2 = self.project_2(x)\n",
    "        projx2 = self.dropout(projx2)\n",
    "        activ2 = F.silu(projx2)\n",
    "\n",
    "        rearranged_proj_x = einops.rearrange(projx1, 'batch seq embed_dim -> batch embed_dim seq')\n",
    "        convoluted_x = self.conv(rearranged_proj_x) # (batch, embed_dim, seq + 3), plus 3 to account for padding\n",
    "        convoluted_reshaped = convoluted_x[:, :, :seq_len]\n",
    "        convoluted_reshaped = einops.rearrange(convoluted_reshaped, 'batch embed_dim seq -> batch seq embed_dim')\n",
    "\n",
    "        # print(\"Conv: \", convoluted_reshaped.shape)\n",
    "\n",
    "        y = self.s6_block(F.silu(convoluted_reshaped))\n",
    "\n",
    "        prod = projx1 * activ2\n",
    "\n",
    "        res = self.project_3(self.dropout(y + prod))\n",
    "        res = self.dropout(res)\n",
    "        return res + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n",
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# test the MambaBlock\n",
    "\n",
    "outer_dim = 3\n",
    "\n",
    "testX = torch.randn(2, 3, outer_dim)\n",
    "print(testX.shape)\n",
    "\n",
    "block = MambaBlock(outer_dim, 4, 5, 2, 3)\n",
    "print(block(testX).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mamba(nn.Module):\n",
    "    def __init__(self, num_layers, outer_dim, embed_dim, latent_dim, d, kernel_size, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([MambaBlock(outer_dim, embed_dim, latent_dim, d, kernel_size, dropout = dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[-0.6640,  0.8465, -1.1752,  0.4254,  0.4553, -0.3023],\n",
      "         [ 0.3246,  0.6437,  0.8164,  0.5838,  0.7913,  2.2071],\n",
      "         [-3.5817, -5.7027, -3.3743, -4.1755,  2.1489, -4.2479]],\n",
      "\n",
      "        [[-0.9033,  1.3857,  0.7061, -3.4100,  0.3220, -1.4397],\n",
      "         [-0.8605,  2.1285,  2.0862,  1.6832, -1.5612,  2.0195],\n",
      "         [ 0.6904, -1.8832, -0.5435, -1.3394, -1.7003, -2.3994]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "# test mamba\n",
    "params = {\n",
    "    'num_layers': 2,\n",
    "    'outer_dim': 6,\n",
    "    'embed_dim': 10,\n",
    "    'latent_dim': 5,\n",
    "    'd': 2,\n",
    "    'kernel_size': 3,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "testX = torch.randn(2, 3, params['outer_dim'])\n",
    "print(testX.shape)\n",
    "\n",
    "mamba = Mamba(**params)\n",
    "print(mamba(testX))\n",
    "print(mamba(testX).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.randn(1000, 100, 6) # (batch, seq_len, outer_dim)\n",
    "y_train = torch.randn(1000, 1, 6) # (batch, seq_len, outer_dim), seq_len = 1 because we just want to predict the next token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_layers': 2,\n",
    "    'outer_dim': 8,\n",
    "    'embed_dim': 10,\n",
    "    'latent_dim': 5,\n",
    "    'd': 2,\n",
    "    'kernel_size': 3,\n",
    "    'dropout': 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2680,  0.0076, -1.0599, -0.2316,  1.4615,  1.4685]],\n",
      "\n",
      "        [[-0.6955, -1.1703, -0.2919, -0.9360, -1.2141,  0.3643]],\n",
      "\n",
      "        [[ 0.3112, -1.2425, -1.5447, -0.0934, -0.3744, -0.4681]],\n",
      "\n",
      "        [[ 0.8679, -1.5998,  0.9578, -0.4581,  1.2563, -1.3466]],\n",
      "\n",
      "        [[ 1.1374,  0.4101,  0.5781,  0.2133, -1.2769,  1.4528]],\n",
      "\n",
      "        [[ 0.6061,  0.4450, -1.1124, -0.8980, -0.8381, -0.1683]],\n",
      "\n",
      "        [[ 1.6411, -0.0721, -0.0734, -1.1112,  0.2307,  0.2016]],\n",
      "\n",
      "        [[ 0.0582, -0.7222, -0.0529,  2.6133,  0.9676, -0.0037]],\n",
      "\n",
      "        [[ 1.0114,  1.1192, -0.3585, -0.1496, -1.3830, -1.7880]],\n",
      "\n",
      "        [[-1.0803, -2.2784,  0.9551,  0.1211, -0.3160, -2.7753]]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "260.6031894683838\n",
      "Epoch 0, batch 0: Loss = 2.606031894683838\n",
      "tensor([[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]]], grad_fn=<SliceBackward0>)\n",
      "nan\n",
      "tensor([[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]]], grad_fn=<SliceBackward0>)\n",
      "nan\n",
      "tensor([[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]]], grad_fn=<SliceBackward0>)\n",
      "nan\n",
      "tensor([[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]]], grad_fn=<SliceBackward0>)\n",
      "nan\n",
      "tensor([[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]]], grad_fn=<SliceBackward0>)\n",
      "nan\n",
      "tensor([[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]]], grad_fn=<SliceBackward0>)\n",
      "nan\n",
      "tensor([[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]]], grad_fn=<SliceBackward0>)\n",
      "nan\n",
      "tensor([[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]]], grad_fn=<SliceBackward0>)\n",
      "nan\n",
      "tensor([[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan]]], grad_fn=<SliceBackward0>)\n",
      "nan\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = optim.Adam(mamba.parameters(), lr = 0.001)\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 10\n",
    "\n",
    "running_loss = []\n",
    "for i in range(num_epochs):\n",
    "    for j in range(0, x_train.size(1), batch_size):\n",
    "        x_batch = x_train[j:j + batch_size]\n",
    "        y_batch = y_train[j:j + batch_size]\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = mamba(x_batch)[:,:1]\n",
    "        print(y_pred)\n",
    "        loss = F.mse_loss(y_pred, y_batch)\n",
    "        print(100 * loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "        if j % 100 == 0:\n",
    "            print(f\"Epoch {i}, batch {j}: Loss = {loss.item()}\")\n",
    "\n",
    "\n",
    "plt.plot(running_loss)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
